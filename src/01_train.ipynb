{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fbce10-681e-4e46-bfa1-62a7bccc04b6",
   "metadata": {},
   "source": [
    "# Vision Transformers for Malaria Detection\n",
    "\n",
    "This set of notebooks was built from the code provided by in Sovit Ranjan Rath's post here: https://debuggercafe.com/malaria-classification-with-vision-transformer-and-pytorch/\n",
    "\n",
    "Much of the text quotes from the blog post.\n",
    "\n",
    "![Cover image from Sovit's post on Debugger Cafe showing sample images of malaria and the attention maps](images/Malaria-Classification-with-Vision-Transformer-and-PyTorch-e1693359747566.png)\n",
    "\n",
    "The dataset is downloaded from Kaggle: [https://www.kaggle.com/datasets/junelsolis/bioimage-informatics-ii-malaria-dataset](https://www.kaggle.com/datasets/junelsolis/bioimage-informatics-ii-malaria-dataset)\n",
    "\n",
    "## Training the model\n",
    "\n",
    "I converted the `tools/train_classifier.py` script to run in this notebook. A few small tweaks to the modules were needed.\n",
    "\n",
    "From the blog:\n",
    "\n",
    "> In this article, we are going to carry out malaria classification with vision transformer and PyTorch. While malaria can be detected from blood samples and laboratory testing, we can speed up the process using deep learning and computer vision. Malaria can be life-threatening if not diagnosed on time. With deep learning, we can train an image classification model that can recognize whether a zoomed-in blood-smear sample has malaria parasites or not. For this malaria classification model, we will employ a vision transformer and the PyTorch framework.\n",
    "> \n",
    "> **Note that 5 plasmodium species cause malaria in humans.** However, in this article, we will classify whether a blood sample contains a malarial parasite or not. We will not distinguish between the different species of Plasmodium. This is just getting started with malaria classification with vision transformer, so, we keep the problem statement simple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b49e22-38f4-4659-9aad-51a3001b2ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tools.utils.dataloaders import get_dataloaders\n",
    "from tools.utils.general import (\n",
    "    SaveBestModel, \n",
    "    save_model,\n",
    "    set_training_dir,\n",
    "    save_loss_plot,\n",
    "    save_accuracy_plot\n",
    ")\n",
    "from tools.utils.load_model import create_model\n",
    "from tools.utils.logging import set_log, log\n",
    "from vision_transformers.models import vit\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc46c4d-e06d-4aeb-95ae-e0fa59f93998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'vit_ti_p16_224'\n",
    "data_dir = '/lustre/fs0/bsc4892/share/Malaria_dataset/train'\n",
    "epochs = 5 \n",
    "lr = 0.0005\n",
    "batch = 128 \n",
    "name = 'vit_ti_5e_128b'\n",
    "valid_split = 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05353e65-2363-4bc7-922d-6a1e22875fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function.\n",
    "def train(model, trainloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    log('Training')\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    counter = 0\n",
    "    for i, data in tqdm(enumerate(trainloader), total=len(trainloader)):\n",
    "        counter += 1\n",
    "        image, labels = data\n",
    "        image = image.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass.\n",
    "        outputs = model(image)\n",
    "        # Calculate the loss.\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()\n",
    "        # Calculate the accuracy.\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        train_running_correct += (preds == labels).sum().item()\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the weights.\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = train_running_loss / counter\n",
    "    epoch_acc = 100. * (train_running_correct / len(trainloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function.\n",
    "def validate(model, testloader, criterion, class_names):\n",
    "    model.eval()\n",
    "    log('Validation')\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            counter += 1\n",
    "            \n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d143cbc-c5fb-4272-b436-2a98df8d1689",
   "metadata": {},
   "source": [
    "## The ViT Tiny Model\n",
    "\n",
    "> For training on the malaria classification dataset, we will use the ViT Tiny model. In the library, we refer to this model as vit_ti_p16_224. The naming convention lets us know that the ViT Tiny model converts each 224×224 image into 16×16 patches. As the dataset is quite simple, we do not need to use any larger model right away. The model has been pretrained on the ImageNet weights.\n",
    ">\n",
    "> For 2 classes, the final model contains roughly 5.5 million parameters. Compared to the base ViT, the ViT Tiny model has a smaller embedding layer. It contains a 192-dimensional embedding instead of a 768-dimensional embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5936303-5db4-440a-951f-565ccf45073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = set_training_dir(name)\n",
    "set_log(OUT_DIR)\n",
    "\n",
    "dataset_train, \\\n",
    "    dataset_valid, \\\n",
    "    train_loader, \\\n",
    "    valid_loader, dataset_classes = get_dataloaders(\n",
    "        data_dir=data_dir,\n",
    "        valid_split=valid_split,\n",
    "        batch_size=batch,\n",
    "        image_size=224\n",
    "    )\n",
    "print(f\"[INFO]: Number of training images: {len(dataset_train)}\")\n",
    "print(f\"[INFO]: Number of validation images: {len(dataset_valid)}\")\n",
    "print(f\"[INFO]: Classes: {dataset_classes}\")\n",
    "\n",
    "# Load the training and validation data loaders.\n",
    "\n",
    "# Learning_parameters. \n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Computation device: {device}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"Epochs to train for: {epochs}\\n\")\n",
    "\n",
    "# Load the model.\n",
    "build_model = create_model[model]\n",
    "model = build_model(\n",
    "    image_size=224, num_classes=len(dataset_classes), pretrained=True\n",
    ")\n",
    "_ = model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "# Loss function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize SaveBestModel class\n",
    "save_best_model = SaveBestModel()\n",
    "\n",
    "# Lists to keep track of losses and accuracies.\n",
    "train_loss, valid_loss = [], []\n",
    "train_acc, valid_acc = [], []\n",
    "\n",
    "# if torch.__version__ >= '2.0.0':\n",
    "#     model = torch.compile(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402a0df-2319-4a1e-ae95-4ea6221e009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training.\n",
    "for epoch in range(epochs):\n",
    "    print(f\"[INFO]: Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_acc = train(model, train_loader, \n",
    "                                            optimizer, criterion)\n",
    "    valid_epoch_loss, valid_epoch_acc = validate(model, valid_loader,  \n",
    "                                                criterion, dataset_classes)\n",
    "    # Save the best model till now.\n",
    "    save_best_model(\n",
    "        model, valid_epoch_loss, epoch, OUT_DIR\n",
    "    )\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    valid_loss.append(valid_epoch_loss)\n",
    "    train_acc.append(train_epoch_acc)\n",
    "    valid_acc.append(valid_epoch_acc)\n",
    "    # Save loss and accuracy plots.\n",
    "    save_loss_plot(OUT_DIR, train_loss, valid_loss)\n",
    "    save_accuracy_plot(OUT_DIR, train_acc, valid_acc)\n",
    "    save_model(OUT_DIR, epoch, model, optimizer, criterion)\n",
    "    print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n",
    "    print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n",
    "    print('-'*50)\n",
    "    \n",
    "print('TRAINING COMPLETE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42434243-7832-4bd9-b5a1-73c5e9bb95b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Biology",
   "language": "python",
   "name": "aibio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
