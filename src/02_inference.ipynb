{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "\n",
    "from PIL import Image\n",
    "from vision_transformers.models import vit\n",
    "from tools.utils.transforms import get_valid_transform\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f764e0a-3e7e-4254-9efe-9b883680b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'parasitized',\n",
    "    'uninfected'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit.vit_ti_p16_224(num_classes=len(class_names), pretrained=False).eval()\n",
    "ckpt = torch.load('runs/training/vit_ti_5e_128b/best_model.pth')\n",
    "model.load_state_dict(ckpt['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean = [0.485, 0.456, 0.406],\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "782ccb04-0fcd-436e-975f-b74649eb9889",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af21ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((224, 224))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "    \n",
    "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "    probabilities = probabilities.numpy()\n",
    "    category = class_names[np.argmax(probabilities)]\n",
    "    plt.text(x=10, y=20, s=category, fontsize='large', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0689d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob('/lustre/fs0/bsc4892/share/Malaria_dataset/test/parasitized/*')\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    if i == 10:\n",
    "        break\n",
    "    infer(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42410ece-4159-4f16-936f-4d1e39d06b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob('/lustre/fs0/bsc4892/share/Malaria_dataset/test/uninfected/*')\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    if i == 10:\n",
    "        break\n",
    "    infer(image_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f809bd2e-1f1a-472a-965b-18e067621368",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a905e-4570-411e-8b6e-a59861fd809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset.\n",
    "dataset_test = datasets.ImageFolder(\n",
    "    '/lustre/fs0/bsc4892/share/Malaria_dataset/test', \n",
    "    transform=(get_valid_transform(224))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e2ca02-0dd1-4800-99e9-c3a490f73672",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of test samples: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d0c89f-87f4-444b-af76-a05c06772e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=128,\n",
    "        num_workers=4,\n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d745f7-8fe8-41aa-bdd8-5b7d292b8aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ca06a-8f1f-4b73-b5bd-a696fcd9e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testloader, criterion):\n",
    "    model.eval().to(device)\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            counter += 1\n",
    "            \n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "        \n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100. * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922467f9-9fff-44a2-a254-5d6c66463cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = validate(model, test_dataloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7ee9ae-d6b9-4bb8-a235-1f78547c179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Test loss: {test_loss:.3f}, test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b6807e4-46f0-4f31-adfb-851ebc336e32",
   "metadata": {},
   "source": [
    "## Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b317d7c-9d16-4fa3-a2dd-bc3d7bf15304",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b1147",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('/lustre/fs0/bsc4892/share/Malaria_dataset/test/parasitized/C100P61ThinF_IMG_20150918_144823_cell_161.png')\n",
    "image = image.resize((224, 224))\n",
    "input_tensor = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feed525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch embedding.\n",
    "patches = model.patches.patch(input_tensor)\n",
    "print(f\"Input tensor shape: {input_tensor.shape}\")\n",
    "print(f\"Patch embedding shape: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8668a6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "fig.suptitle(\"Image patches\", fontsize=12)\n",
    "img = np.asarray(image)\n",
    "for i in range(0, 196):\n",
    "    x = i % 14\n",
    "    y = i // 14\n",
    "    patch = img[y*16:(y+1)*16, x*16:(x+1)*16]\n",
    "    ax = fig.add_subplot(14, 14, i+1)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.imshow(patch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df57d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed = model.pos_embedding\n",
    "print(pos_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776083d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_input = patches.view(1, 192, 196).permute(0, 2, 1)\n",
    "print(patch_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ca5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_input = torch.cat((model.cls_token, patch_input), dim=1) + pos_embed\n",
    "print(\"Transformer input: \", transformer_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080aed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_input_qkv = model.transformer.layers[0][0].fn.qkv(transformer_input)[0]\n",
    "print(transformer_input_qkv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30351e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = transformer_input_qkv.reshape(197, 3, 12, 16)\n",
    "print(\"Reshaped qkv : \", qkv.shape)\n",
    "q = qkv[:, 0].permute(1, 0, 2)\n",
    "k = qkv[:, 1].permute(1, 0, 2)\n",
    "kT = k.permute(0, 2, 1)\n",
    "print(\"K transposed: \", kT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcc1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Matrix\n",
    "attention_matrix = q @ kT\n",
    "print(\"Attention matrix: \", attention_matrix.shape)\n",
    "plt.imshow(attention_matrix[3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728eeea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention matrix\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "fig.suptitle(\"Attention Maps\", fontsize=20)\n",
    "# fig.add_axes()\n",
    "img = np.asarray(img)\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "ax1.imshow(img)\n",
    "ax1.axis('off')\n",
    "fig = plt.figure(figsize=(16, 8))\n",
    "for i in range(8):\n",
    "    attn_heatmap = attention_matrix[i, 64, 1:].reshape((14, 14)).detach().cpu().numpy()\n",
    "    ax2 = fig.add_subplot(2, 4, i+1)\n",
    "    ax2.imshow(attn_heatmap)\n",
    "    ax2.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4089ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Biology",
   "language": "python",
   "name": "aibio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
